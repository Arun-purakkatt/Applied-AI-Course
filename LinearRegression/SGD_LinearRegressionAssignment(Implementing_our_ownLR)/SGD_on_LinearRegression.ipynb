{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using Stochastic Gradient Descent(SGD) on Boston House Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import r2_score\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: There are 506 Data points with 13 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = boston_data.target\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "x_train = std.fit_transform(x_train)\n",
    "x_test = std.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Linear Regression using Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegressionGD(X,y,w = np.zeros(13),b = 0,iterations=5000,learning_rate=0.1,epsilon_val=0.004):\n",
    "    \"\"\"\n",
    "        Linear Regression Using Gradient Descent Optimization\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array or sparse matrix of shape [n_samples,n_features]\n",
    "            Training data\n",
    "\n",
    "        y : numpy array of shape [n_samples, n_targets]\n",
    "            Target values\n",
    "\n",
    "        w:  Weight Vector first initializad with 0's\n",
    "        \n",
    "        \n",
    "        b:  y-intercept\n",
    "        \n",
    "        \n",
    "        iterations: no.of iterations\n",
    "        \n",
    "        epsilon_val: Termination Criteria\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        w,b,cost or MSE\n",
    "        \"\"\"\n",
    "    start = datetime.now()\n",
    "    n = float(len(y))\n",
    "    costs = [];\n",
    "    decay_rate = 0.0001\n",
    "    eta=learning_rate\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        #learning rate \n",
    "        eta = eta * (1.0/(1+decay_rate*i))\n",
    "        error = y -  predict(X,w,b) #y_act - y_actual\n",
    "        \n",
    "        #partial differentiation w.r.t w\n",
    "        w_grad = (-2.0/n)*(X.T.dot(error))\n",
    "        \n",
    "        #partial differentiation w.r.t b\n",
    "        b_grad = (-2.0/n)*(np.sum(error))\n",
    "        \n",
    "        #MSE\n",
    "        cost = (1.0/n)*np.sum(np.power(error,2))\n",
    "        \n",
    "        #updating weight vector\n",
    "        w = w - (eta*w_grad)\n",
    "        \n",
    "        #updating y-intercept\n",
    "        b = b - (eta*b_grad)\n",
    "        \n",
    "        costs.append(cost)\n",
    "        \n",
    "        #Stopping Criteria\n",
    "        if i==0:\n",
    "            w_prev = w;\n",
    "        else:\n",
    "            dist = (np.linalg.norm(w-w_prev))\n",
    "            w_prev = w\n",
    "            \n",
    "            #Terminates the loop when difference between w and w_prev is very less \n",
    "            if(round(dist,6)<epsilon_val):\n",
    "                print(\"no.of iterations: \",i,\"Cost: \",min(costs))\n",
    "                break;\n",
    "        \n",
    "    print(\"Time: Taken: \",datetime.now() - start)\n",
    "    return w,b,cost;\n",
    "\n",
    "\n",
    "\n",
    "def predict(x,m,c):\n",
    "    \"\"\"\n",
    "        It predicts the target value based on the input parameter values\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy array or sparse matrix of shape [n_samples,n_features]\n",
    "            Training data\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    y = np.dot(x,m) + c\n",
    "    return y\n",
    "\n",
    "\n",
    "def score(X,y,m,c):\n",
    "    \n",
    "    \"\"\"\n",
    "    It gives the Score or accuracy\n",
    "    \n",
    "    Parameters\n",
    "        ----------\n",
    "        X : numpy array or sparse matrix of shape [n_samples,n_features]\n",
    "            Training data\n",
    "            \n",
    "        y : numpy array of shape [n_samples, n_targets]\n",
    "            Target values\n",
    "            \n",
    "        m : slope\n",
    "        \n",
    "        c : y-intercept  \n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.metrics import r2_score\n",
    "    return r2_score(y,predict(X,m,c).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no.of iterations:  152 Cost:  23.1395328035\n",
      "Time: Taken:  0:00:00.011968\n"
     ]
    }
   ],
   "source": [
    "w1,b1,cost1 = linearRegressionGD(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72196287871389897"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(x_test,y_test,w1,b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Linear Regression using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegressionSGD(X,y,w = np.zeros(13),b = 0,iterations=1000,case_size=100,learning_rate=0.1,epsilon_val=0.004):\n",
    "    \"\"\"\n",
    "        Linear Regression Using Gradient Descent Optimization\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array or sparse matrix of shape [n_samples,n_features]\n",
    "            Training data\n",
    "\n",
    "        y : numpy array of shape [n_samples, n_targets]\n",
    "            Target values\n",
    "\n",
    "        w:  Weight Vector first initializad with 0's\n",
    "        \n",
    "        \n",
    "        b:  y-intercept\n",
    "        \n",
    "        \n",
    "        iterations: no.of iterations\n",
    "        \n",
    "        \n",
    "        case_size : bunch of points to be taken in each iteration \n",
    "        \n",
    "        \n",
    "        epsilon_val: Termination Criteria\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        w,b,cost or MSE\n",
    "        \"\"\"\n",
    "    start = datetime.now()\n",
    "    n = float(len(y))\n",
    "    costs = [];\n",
    "    decay_rate = 0.0001\n",
    "    eta=learning_rate\n",
    "    \n",
    "    \n",
    "    for iters in range(iterations):\n",
    "        eta = eta * (1.0/(1+decay_rate*iters))\n",
    "        \n",
    "        X,y =  shuffle(X,y,random_state=0,replace=False)\n",
    "    \n",
    "    \n",
    "        for i in range(case_size):\n",
    "            x_mini = X[i:i+case_size]\n",
    "            y_mini = y[i:i+case_size]\n",
    "            #learning rate \n",
    "            #eta = eta * (1.0/(1+decay_rate*i))\n",
    "            error = y_mini -  predict(x_mini,w,b) #y_act - y_predicted\n",
    "\n",
    "            #partial differentiation w.r.t w\n",
    "            w_grad = (-2.0/n)*(x_mini.T.dot(error))\n",
    "\n",
    "            #partial differentiation w.r.t b\n",
    "            b_grad = (-2.0/n)*(np.sum(error))\n",
    "\n",
    "            #MSE\n",
    "            cost = (1.0/n)*np.sum(np.power(error,2))\n",
    "\n",
    "            #updating weight vector\n",
    "            w = w - (eta*w_grad)\n",
    "\n",
    "            #updating y-intercept\n",
    "            b = b - (eta*b_grad)\n",
    "\n",
    "            costs.append(cost)\n",
    "\n",
    "            #Stopping Criteria\n",
    "        #Stopping Criteria\n",
    "        if iters==0:\n",
    "            w_prev = w;\n",
    "        else:\n",
    "            dist = (np.linalg.norm(w-w_prev))\n",
    "            w_prev = w\n",
    "            \n",
    "            #Terminates the loop when difference between w and w_prev is very less \n",
    "            if(round(dist,6)<epsilon_val):\n",
    "                print(\"no.of iterations: \",iters,\"Cost: \",min(costs))\n",
    "                break;\n",
    "        \n",
    "    print(\"Time Taken: \",datetime.now() - start)\n",
    "    return w,b,cost;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no.of iterations:  375 Cost:  2.63148090471\n",
      "Time Taken:  0:00:00.915583\n"
     ]
    }
   ],
   "source": [
    "w,b,cost = linearRegressionSGD(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.58435358,  1.1170941 ,  0.5553632 ,  0.86510863, -2.0640718 ,\n",
       "        2.65955573,  0.41057385, -3.07033171,  2.77224903, -2.12395843,\n",
       "       -2.001628  ,  1.0886039 , -4.51357934])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final Weight Vector\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.782099005287087"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final y-intercept\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.1187871776980671"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72068643671585575"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(x_test,y_test,w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKlearn Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#n_job=no.cores processor, Parallelization\n",
    "lr = LinearRegression(n_jobs=4)\n",
    "\n",
    "lr.fit(x_train,y_train)\n",
    "\n",
    "y_pred = lr.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72237302553200955"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores Custom vs Sklearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom GD Linear Regression Score:  0.721962878714\n",
      "Custom SGD Linear Regression Score:  0.720686436716\n",
      "SkLearn Linear Regression Score:  0.722373025532\n"
     ]
    }
   ],
   "source": [
    "print(\"Custom GD Linear Regression Score: \",score(x_test,y_test,w1,b1))\n",
    "print(\"Custom SGD Linear Regression Score: \",score(x_test,y_test,w,b))\n",
    "print(\"SkLearn Linear Regression Score: \",r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
